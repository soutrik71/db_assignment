{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(300000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/insights-model-run2/code/Users/soutrik.chowdhury/db_assignment\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\n",
    "    \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/insights-model-run2/code/Users/soutrik.chowdhury/db_assignment\"\n",
    ")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandarallel import pandarallel\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from contractions import fix as contractions_fix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Ensure you have downloaded these nltk resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/ubuntu_customer_msg_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>raw_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hello folks, please help me a bit with the following sentence: 'Order here your personal photos or videos.' - I think the only allowed version is 'Order your personal videos or photos here.', but I'm not sure, are you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>is there any reason why my 'network manager' icon in my tray does not scale when the other ones get resized when i change the size of the panel?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I installed the 64bit version of ubuntu and I can't open firefox(segfault) and if I try to open nautilus nothing happens and my cpu goes 100%, what can I do???</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Hello Does Ubuntu have somekind of register to configure applications and os settings?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>anyone else run into issues with cd/dvd burners not identifying blank media installed?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                                                                                                                                                                                    raw_text\n",
       "0           0  Hello folks, please help me a bit with the following sentence: 'Order here your personal photos or videos.' - I think the only allowed version is 'Order your personal videos or photos here.', but I'm not sure, are you?\n",
       "1           1                                                                            is there any reason why my 'network manager' icon in my tray does not scale when the other ones get resized when i change the size of the panel?\n",
       "2           2                                                             I installed the 64bit version of ubuntu and I can't open firefox(segfault) and if I try to open nautilus nothing happens and my cpu goes 100%, what can I do???\n",
       "3           3                                                                                                                                      Hello Does Ubuntu have somekind of register to configure applications and os settings?\n",
       "4           4                                                                                                                                      anyone else run into issues with cd/dvd burners not identifying blank media installed?"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(185762, 1)\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns=[\"Unnamed: 0\"], axis=1)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_text    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(10).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Text cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Preprocess the text\"\"\"\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    # Handle contractions\n",
    "    text = contractions_fix(text)\n",
    "\n",
    "    # Remove links\n",
    "    text = re.sub(r\"http\\S+\", \"http-link\", text)\n",
    "\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r\"[^A-Za-z0-9\\s/?/./']\", \"\", text)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [\n",
    "        word\n",
    "        for word in tokens\n",
    "        if word not in stop_words\n",
    "        or word\n",
    "        in [\n",
    "            \"what\",\n",
    "            \"how\",\n",
    "            \"why\",\n",
    "            \"where\",\n",
    "            \"do\",\n",
    "            \"does\",\n",
    "            \"did\",\n",
    "            \"is\",\n",
    "            \"does\",\n",
    "            \"do\",\n",
    "            \"can\",\n",
    "            \"could\",\n",
    "            \"would\",\n",
    "            \"should\",\n",
    "            \"not\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Reconstruct the text\n",
    "    text = \" \".join(tokens)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample[\"clean_text\"] = sample[\"raw_text\"].parallel_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f183d46c6cc453a904796b462817944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=30961), Label(value='0 / 30961')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-6c4d41af8d1b>:4: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n",
      "<ipython-input-10-6c4d41af8d1b>:4: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n",
      "<ipython-input-10-6c4d41af8d1b>:4: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n",
      "<ipython-input-10-6c4d41af8d1b>:4: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n",
      "<ipython-input-10-6c4d41af8d1b>:4: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n",
      "<ipython-input-10-6c4d41af8d1b>:4: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    }
   ],
   "source": [
    "df[\"clean_text\"] = df[\"raw_text\"].parallel_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(185762, 2)\n",
      "raw_text      0\n",
      "clean_text    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_sentences = sample[\"clean_text\"].tolist()\n",
    "# print(len(dummy_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_encoder(sentence: str, bert_tokenizer: BertTokenizer):\n",
    "    \"\"\"Encode the input sentence using the BERT tokenizer\"\"\"\n",
    "\n",
    "    indexed_tokens = bert_tokenizer.encode(\n",
    "        sentence,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    segments = torch.ones_like(indexed_tokens)\n",
    "\n",
    "    return indexed_tokens, segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_output(\n",
    "    bert_model: BertModel, indexed_tokens: torch.Tensor, segments: torch.Tensor\n",
    "):\n",
    "    \"\"\"Get the model output for the input tokens\"\"\"\n",
    "\n",
    "    bert_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(indexed_tokens, token_type_ids=segments)\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    token_embeddings = torch.stack(\n",
    "        hidden_states, dim=0\n",
    "    )  # (layer, batch, token, feature)\n",
    "\n",
    "    return token_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word  and sentence token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(token_embeddings: torch.Tensor):\n",
    "    \"\"\"Get the sentence embedding from the token embeddings\"\"\"\n",
    "    token_embeddings = token_embeddings.squeeze(1)  #  (layer, token, feature)\n",
    "    token_embeddings = token_embeddings.permute(1, 0, 2)  # (token, layer, feature)\n",
    "    # Stores the token vectors, with shape [seq_len x features]\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # `token_embeddings` is a [seq_len, layer, feature] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [12 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "\n",
    "    print(\"Shape is: %d x %d\" % (len(token_vecs_sum), len(token_vecs_sum[0])))\n",
    "    sentence_embedding = torch.stack(token_vecs_sum).mean(dim=0).flatten()\n",
    "    print(\"Final shape of sentence is: \", sentence_embedding.shape)\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding_opt(token_embeddings: torch.Tensor):\n",
    "    \"\"\"Get the sentence embedding from the token embeddings\"\"\"\n",
    "    # Sum the last four layers\n",
    "    sum_last_four_layers = torch.sum(\n",
    "        token_embeddings[-4:], dim=0\n",
    "    )  # (batch_size, seq_len, feature_dim)\n",
    "    print(\"Shape of sum_last_four_layers: \", sum_last_four_layers.shape)\n",
    "    # Mean of all tokens in the sentence\n",
    "    sentence_embedding = torch.mean(\n",
    "        sum_last_four_layers, dim=1\n",
    "    )  # (batch_size, feature_dim)\n",
    "    sentence_embedding = sentence_embedding.squeeze(0)\n",
    "    print(\"Shape of sentence_embedding: \", sentence_embedding.shape)\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Create a sample input tensor (for testing)\n",
    "# N_layers = 12\n",
    "# batch_size = 1\n",
    "# seq_len = 10\n",
    "# feature_dim = 768\n",
    "# sample_input = torch.randn(N_layers, batch_size, seq_len, feature_dim)\n",
    "\n",
    "# # Test the original function\n",
    "# output1 = get_sentence_embedding(sample_input.clone())\n",
    "\n",
    "# # Test the optimized function\n",
    "# output2 = get_sentence_embedding_opt(sample_input.clone())\n",
    "\n",
    "# # Compare the results\n",
    "# print(torch.allclose(output1, output2, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each token of our input we have 13 separate vectors each of length 768 \n",
    "In order to get the individual vectors we will need to combine some of the layer vectors…but which layer or combination of layers provides the best representation\n",
    "let’s try creating the word vectors by summing together the last four layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the entire sentence what we can do is to take the mean of the last 4 layers for each token and then take the mean of all the tokens to get a single vector representation of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_txt = sample[\"clean_text\"].iloc[5]\n",
    "# print(demo_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexed_tokens, segments = data_encoder(demo_txt, bert_tokenizer)\n",
    "# token_embeddings = model_output(bert_model, indexed_tokens, segments)\n",
    "# sentence_embedding = get_sentence_embedding(token_embeddings)\n",
    "# sentence_embedding_2 = get_sentence_embedding_opt(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(\n",
    "    sentence: str, bert_tokenizer: BertTokenizer, bert_model: BertModel\n",
    "):\n",
    "    \"\"\"Get the BERT embedding for the input sentence\"\"\"\n",
    "\n",
    "    indexed_tokens, segments = data_encoder(sentence, bert_tokenizer)\n",
    "    token_embeddings = model_output(bert_model, indexed_tokens, segments)\n",
    "    sentence_embedding = get_sentence_embedding_opt(token_embeddings)\n",
    "\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(\n",
    "    sentence: str, bert_tokenizer: BertTokenizer, bert_model: BertModel\n",
    "):\n",
    "    \"\"\"Process a single sentence and return the sentence and its embedding\"\"\"\n",
    "    embedding = get_bert_embedding(sentence, bert_tokenizer, bert_model)\n",
    "    return sentence, embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def process_sentences(\n",
    "#     sentences: list, bert_tokenizer: BertTokenizer, bert_model: BertModel\n",
    "# ):\n",
    "#     \"\"\"Process a list of sentences in parallel and store results in a dictionary\"\"\"\n",
    "#     results = {}\n",
    "#     with ThreadPoolExecutor() as executor:\n",
    "#         future_to_sentence = {\n",
    "#             executor.submit(\n",
    "#                 process_sentence, sentence, bert_tokenizer, bert_model\n",
    "#             ): sentence\n",
    "#             for sentence in sentences\n",
    "#         }\n",
    "#         for future in as_completed(future_to_sentence):\n",
    "#             sentence = future_to_sentence[future]\n",
    "#             try:\n",
    "#                 _, embedding = future.result()\n",
    "#                 results[sentence] = embedding\n",
    "#             except Exception as exc:\n",
    "#                 print(f\"{sentence} generated an exception: {exc}\")\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences(\n",
    "    sentences: list, bert_tokenizer: BertTokenizer, bert_model: BertModel, n_jobs=-1\n",
    "):\n",
    "    \"\"\"Process a list of sentences in parallel and store results in a dictionary\"\"\"\n",
    "    # Create a partial function with bert_tokenizer and bert_model\n",
    "    process_with_model = partial(\n",
    "        process_sentence, bert_tokenizer=bert_tokenizer, bert_model=bert_model\n",
    "    )\n",
    "    results = Parallel(n_jobs=n_jobs, timeout=100)(\n",
    "        delayed(process_with_model)(sentence) for sentence in sentences\n",
    "    )\n",
    "    return dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185762\n"
     ]
    }
   ],
   "source": [
    "all_intents = df[\"clean_text\"].tolist()\n",
    "print(len(all_intents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/torch_env/lib/python3.10/site-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid = os.fork()\n",
      "/anaconda/envs/torch_env/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "all_intents_embeddings = process_sentences(all_intents, bert_tokenizer, bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # apply all these functions to the column cleantext of the df\n",
    "# df[\"bert_embedding\"] = df[\"clean_text\"].apply(\n",
    "#     lambda x: get_bert_embedding(x, bert_tokenizer, bert_model)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./data/pre_processed\", exist_ok=True)\n",
    "df.to_csv(\n",
    "    \"./data/pre_processed/ubuntu_customer_msg_small_pre_processed_bert_base.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cls_bert = np.vstack(sample['bert_embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cls_bert.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_embeddings(embeddings, num_clusters):\n",
    "    scaler = StandardScaler()\n",
    "    normalized_embeddings = scaler.fit_transform(embeddings)\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "    clusters = kmeans.fit_predict(normalized_embeddings)\n",
    "    return clusters, kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clustering(embeddings, clusters):\n",
    "    silhouette_avg = silhouette_score(embeddings, clusters)\n",
    "    return silhouette_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
